# File: static/robots.txt
# allow crawling everything by default

User-agent: *
Disallow:
